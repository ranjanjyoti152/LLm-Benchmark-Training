#!/bin/bash
# Simple run script for the LLM benchmarking tool

echo "ğŸš€ LLM Multi-GPU Benchmarking Tool"
echo "=================================="
echo

# Check if virtual environment exists
if [ ! -d "venv" ] && [ ! -d ".venv" ]; then
    echo "âš ï¸  No virtual environment detected."
    echo "Creating virtual environment..."
    python3 -m venv venv
    source venv/bin/activate
    echo "ğŸ“¦ Installing dependencies..."
    pip install -r requirements.txt
else
    # Activate existing environment
    if [ -d "venv" ]; then
        source venv/bin/activate
    elif [ -d ".venv" ]; then
        source .venv/bin/activate
    fi
fi

# Check if dependencies are installed
if ! python -c "import torch" 2>/dev/null; then
    echo "ğŸ“¦ Installing missing dependencies..."
    pip install -r requirements.txt
fi

echo "ğŸ” Checking system requirements..."
python benchmark_cli.py check-system

echo
echo "Choose an option:"
echo "1) Run full benchmark (all models)"
echo "2) Run quick benchmark (100 samples)"
echo "3) Check system requirements"
echo "4) Generate configuration file"
echo "5) Custom run (specify options)"
echo "6) Exit"
echo

read -p "Enter your choice (1-6): " choice

case $choice in
    1)
        echo "ğŸ¯ Running full benchmark..."
        python benchmark_cli.py run
        ;;
    2)
        echo "âš¡ Running quick benchmark..."
        python benchmark_cli.py run --quick
        ;;
    3)
        echo "ğŸ” System requirements check..."
        python benchmark_cli.py check-system --detailed
        ;;
    4)
        echo "ğŸ“ Generating configuration file..."
        python benchmark_cli.py generate-config --output my_config.yaml
        ;;
    5)
        echo "ğŸ® Custom benchmark options:"
        echo "Available models: 1B, 7B, 20B, 120B"
        read -p "Enter model sizes (space-separated): " models
        read -p "Enter number of samples (default 1000): " samples
        read -p "Enter output directory (default ./benchmark_results): " output
        
        # Set defaults
        samples=${samples:-1000}
        output=${output:-./benchmark_results}
        
        echo "ğŸ¯ Running custom benchmark..."
        python benchmark_cli.py run --models $models --samples $samples --output "$output"
        ;;
    6)
        echo "ğŸ‘‹ Goodbye!"
        exit 0
        ;;
    *)
        echo "âŒ Invalid choice. Please try again."
        exit 1
        ;;
esac

echo
echo "âœ… Done! Check the output directory for results."